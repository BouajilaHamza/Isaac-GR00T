{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Gr00tPolicy to ONNX\n",
    "\n",
    "This notebook demonstrates how to export the core neural network of a `Gr00tPolicy` to the ONNX (Open Neural Network Exchange) format using the `to_onnx` method.\n",
    "\n",
    "**Why ONNX?**\n",
    "*   **Interoperability:** Run your model in different frameworks and runtimes (ONNX Runtime, TensorRT, OpenVINO).\n",
    "*   **Optimization:** ONNX runtimes often provide hardware-specific optimizations for faster inference.\n",
    "*   **Deployment:** Simplifies deploying models to various platforms (servers, edge devices).\n",
    "\n",
    "**Key Considerations:**\n",
    "*   **Preprocessing/Postprocessing:** The `to_onnx` method exports *only* the underlying `torch.nn.Module` (`policy.model`). The data normalization (`policy.apply_transforms`) and denormalization (`policy.unapply_transforms`) steps are **NOT** included in the ONNX graph. You **MUST** reimplement this logic in your deployment environment.\n",
    "*   **Input:** The `to_onnx` method requires an example input dictionary that has already been **normalized** using `policy.apply_transforms`.\n",
    "*   **Output:** The ONNX model will output **normalized** actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Policy Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing gr00t components: No module named 'gr00t'\n",
      "Please ensure you have the gr00t library installed and accessible,\n",
      "and replace placeholder imports with your actual class paths.\n",
      "Error initializing Gr00tPolicy: Dummy class, replace with actual import\n",
      "Please check your model path, embodiment tag, and configurations.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration (Replace with your actual paths and settings) ---\n",
    "# Example using a Hugging Face model ID\n",
    "MODEL_ID = \"nvidia/gr00t-1\" # Or use a local path: \"/path/to/your/gr00t/checkpoint\"\n",
    "EMBODIMENT_TAG = \"franka_panda\" # Specify the embodiment your model uses\n",
    "OUTPUT_ONNX_PATH = \"gr00t_policy_model.onnx\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Mock/Placeholder Imports (Replace with your actual classes) ---\n",
    "# These classes depend on your specific gr00t setup.\n",
    "# You'll need to import them correctly from your project structure.\n",
    "try:\n",
    "    from gr00t.model.policy import Gr00tPolicy\n",
    "    from gr00t.data.dataset import ModalityConfig # Placeholder\n",
    "    from gr00t.data.transform.base import ComposedModalityTransform # Placeholder\n",
    "    # Add any other necessary imports for your specific ModalityConfig/Transform\n",
    "    from gr00t.data.transform.state_action import StateActionToTensor, Normalizer # Example transforms\n",
    "    from gr00t.data.transform import VideoTransform # Example transforms\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing gr00t components: {e}\")\n",
    "    print(\"Please ensure you have the gr00t library installed and accessible,\")\n",
    "    print(\"and replace placeholder imports with your actual class paths.\")\n",
    "    # Define dummy classes if imports fail, to allow notebook structure to load\n",
    "    class ModalityConfig:\n",
    "        def __init__(self, modality_type, delta_indices, **kwargs): pass\n",
    "    class ComposedModalityTransform:\n",
    "        def __init__(self, *args, **kwargs): pass\n",
    "        def __call__(self, obs): return obs # Dummy implementation\n",
    "        def unapply(self, act): return act # Dummy implementation\n",
    "        def set_metadata(self, meta): pass\n",
    "        def eval(self): pass\n",
    "    class Gr00tPolicy:\n",
    "        def __init__(self, *args, **kwargs): raise NotImplementedError(\"Dummy class, replace with actual import\")\n",
    "\n",
    "# --- Define Modality Configuration and Transforms (Crucial Step) ---\n",
    "# This MUST match the configuration used during training and expected by the model.\n",
    "# Replace this with your actual configuration.\n",
    "modality_config = {\n",
    "    # Example: Define expected modalities, horizons, etc.\n",
    "    \"video\": ModalityConfig(modality_type=\"video\", delta_indices=[-1, 0]), # Needs H, W, C etc.\n",
    "    \"state\": ModalityConfig(modality_type=\"state\", delta_indices=[-1, 0]), # Needs state keys\n",
    "    \"action\": ModalityConfig(modality_type=\"action\", delta_indices=list(range(16))), # Needs action keys, action_horizon\n",
    "    # Add other modalities (e.g., text) if your model uses them\n",
    "}\n",
    "\n",
    "# Example: Define the sequence of transforms\n",
    "# Replace with your actual transform pipeline\n",
    "modality_transform = ComposedModalityTransform(\n",
    "    # Example transforms - replace with yours\n",
    "    # ImgTransform(...),\n",
    "    # NormalizeStateTensor(...),\n",
    "    # NormalizeActionTensor(...),\n",
    ")\n",
    "\n",
    "# --- Initialize the Policy ---\n",
    "try:\n",
    "    policy = Gr00tPolicy(\n",
    "        model_path=MODEL_ID,\n",
    "        embodiment_tag=EMBODIMENT_TAG,\n",
    "        modality_config=modality_config,\n",
    "        modality_transform=modality_transform,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    print(f\"Gr00tPolicy initialized successfully on device: {policy.device}\")\n",
    "    print(f\"Model path resolved to: {policy.model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gr00tPolicy: {e}\")\n",
    "    print(\"Please check your model path, embodiment tag, and configurations.\")\n",
    "    policy = None # Set policy to None if initialization fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Example Observation Data\n",
    "\n",
    "We need a sample observation dictionary with the correct structure and data types that the policy expects. The shapes should match your `modality_config` (especially the time dimension based on `delta_indices`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Dummy Observation Data ---\n",
    "# Replace with realistic data or load a sample from your dataset.\n",
    "# The shapes (especially time dimension T) must match policy requirements.\n",
    "\n",
    "# Example: Assuming delta_indices=[-1, 0], so Time dimension T=2\n",
    "T = 2\n",
    "H, W, C = 224, 224, 3 # Example video dimensions\n",
    "state_dim_arm = 7\n",
    "state_dim_hand = 6\n",
    "\n",
    "observations = {\n",
    "    # Use keys defined in your modality_config\n",
    "    'video.agentview_left': np.zeros((T, H, W, C), dtype=np.uint8),\n",
    "    'video.agentview_right': np.zeros((T, H, W, C), dtype=np.uint8),\n",
    "    'state.joint_positions': np.zeros((T, state_dim_arm * 2), dtype=np.float32),\n",
    "    'state.gripper_positions': np.zeros((T, state_dim_hand * 2), dtype=np.float32),\n",
    "    # Add other observation keys as needed by your model/config\n",
    "    # 'language.instruction': ['pick up the cube'] # Example text input\n",
    "}\n",
    "\n",
    "# Helper function from policy.py (or copy it here)\n",
    "def unsqueeze_dict_values(data: dict) -> dict:\n",
    "    unsqueezed_data = {}\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            unsqueezed_data[k] = np.expand_dims(v, axis=0)\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            unsqueezed_data[k] = v.unsqueeze(0)\n",
    "        else:\n",
    "            # Handle non-tensor data like lists of strings (e.g., language)\n",
    "            if isinstance(v, list):\n",
    "                 # Assuming batch size 1 for non-tensor lists\n",
    "                 unsqueezed_data[k] = v\n",
    "            else:\n",
    "                 unsqueezed_data[k] = v # Or handle other types as needed\n",
    "    return unsqueezed_data\n",
    "\n",
    "# Add batch dimension (B=1) as policy methods expect it\n",
    "observations_batched = unsqueeze_dict_values(observations)\n",
    "\n",
    "print(\"Sample observation keys:\", list(observations_batched.keys()))\n",
    "print(\"Example shape (video):\", observations_batched['video.agentview_left'].shape)\n",
    "print(\"Example shape (state):\", observations_batched['state.joint_positions'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalize Input Data\n",
    "\n",
    "This is a **critical step**. The `to_onnx` method requires the input data to be normalized using the policy's `apply_transforms` method. The resulting tensors must also be moved to the same device as the policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if policy is not None:\n",
    "    # Apply the same preprocessing transforms used during training/inference\n",
    "    normalized_input = policy.apply_transforms(observations_batched)\n",
    "\n",
    "    # Ensure all tensors in the normalized input are on the correct device\n",
    "    example_normalized_input_device = {}\n",
    "    for key, value in normalized_input.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            example_normalized_input_device[key] = value.to(policy.device)\n",
    "        else:\n",
    "            # Handle non-tensor data if necessary (e.g., language embeddings might already be tensors)\n",
    "            example_normalized_input_device[key] = value\n",
    "\n",
    "    print(\"Normalized input prepared and moved to device:\", policy.device)\n",
    "    print(\"Normalized input keys:\", list(example_normalized_input_device.keys()))\n",
    "    # Print shape of one tensor to verify\n",
    "    first_tensor_key = next(k for k, v in example_normalized_input_device.items() if isinstance(v, torch.Tensor))\n",
    "    print(f\"Example normalized shape ({first_tensor_key}):\", example_normalized_input_device[first_tensor_key].shape)\n",
    "else:\n",
    "    print(\"Policy not initialized. Skipping normalization.\")\n",
    "    example_normalized_input_device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define ONNX Export Parameters\n",
    "\n",
    "We need to define:\n",
    "*   `input_names`: List of names corresponding to the keys in `example_normalized_input_device`.\n",
    "*   `output_names`: List of names for the output tensors produced by the ONNX model. This depends on the output structure of `policy.model.get_action`. Often, it's a single tensor (e.g., `['action_pred']`).\n",
    "*   `dynamic_axes` (Optional): Allows specifying which dimensions (like batch size) can vary at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if example_normalized_input_device is not None:\n",
    "    # Input names should match the keys of the normalized input dictionary\n",
    "    input_names = list(example_normalized_input_device.keys())\n",
    "\n",
    "    # Output names depend on what policy.model.get_action returns.\n",
    "    # Check the Gr00tPolicy._get_action_from_normalized_input method.\n",
    "    # It likely returns a dict like {'action_pred': tensor}, so we use ['action_pred'].\n",
    "    # Adjust if your model's output structure is different.\n",
    "    output_names = [\"action_pred\"]\n",
    "\n",
    "    # Optional: Define dynamic axes for variable batch size\n",
    "    # This allows the exported ONNX model to handle inputs with different batch sizes.\n",
    "    dynamic_axes = {}\n",
    "    for name in input_names:\n",
    "        # Assuming the first dimension is batch for all tensor inputs\n",
    "        if isinstance(example_normalized_input_device[name], torch.Tensor):\n",
    "             dynamic_axes[name] = {0: 'batch_size'}\n",
    "        # Add handling for other dynamic axes if needed (e.g., sequence length)\n",
    "\n",
    "    for name in output_names:\n",
    "        # Assuming the first dimension is batch for all tensor outputs\n",
    "        dynamic_axes[name] = {0: 'batch_size'}\n",
    "        # Add handling for other dynamic axes if needed (e.g., action horizon)\n",
    "\n",
    "    print(\"Input Names:\", input_names)\n",
    "    print(\"Output Names:\", output_names)\n",
    "    print(\"Dynamic Axes:\", dynamic_axes)\n",
    "else:\n",
    "    print(\"Normalized input not available. Skipping parameter definition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if policy is not None and example_normalized_input_device is not None:\n",
    "    print(f\"Attempting to export model to: {OUTPUT_ONNX_PATH}\")\n",
    "    try:\n",
    "        policy.to_onnx(\n",
    "            output_path=OUTPUT_ONNX_PATH,\n",
    "            example_normalized_input=example_normalized_input_device,\n",
    "            input_names=input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "            opset_version=17, # Try other versions (e.g., 14, 16) if export fails\n",
    "            verbose=True # Set to False for less output\n",
    "        )\n",
    "        print(\"\\nONNX export successful!\")\n",
    "        print(f\"Model saved to {os.path.abspath(OUTPUT_ONNX_PATH)}\")\n",
    "        print(\"\\nReminder: The exported model expects NORMALIZED inputs and produces NORMALIZED outputs.\")\n",
    "        print(\"You need to implement pre-processing (normalization) and post-processing (denormalization) separately in your deployment environment.\")\n",
    "        onnx_export_successful = True\n",
    "    except Exception as e:\n",
    "        print(f\"\\nONNX export failed: {e}\")\n",
    "        # Error message already printed by policy.to_onnx\n",
    "        onnx_export_successful = False\n",
    "else:\n",
    "    print(\"Policy or normalized input not ready. Skipping ONNX export.\")\n",
    "    onnx_export_successful = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification (Optional) - Using ONNX Runtime\n",
    "\n",
    "Let's load the exported ONNX model and run inference using `onnxruntime` to check if the output matches the PyTorch model's output (before denormalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onnx_export_successful:\n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        import onnx\n",
    "    except ImportError:\n",
    "        print(\"Please install onnx and onnxruntime: pip install onnx onnxruntime\")\n",
    "        ort = None\n",
    "\n",
    "    if ort:\n",
    "        try:\n",
    "            print(f\"\\nLoading ONNX model from {OUTPUT_ONNX_PATH}...\")\n",
    "            # Check model validity\n",
    "            onnx_model = onnx.load(OUTPUT_ONNX_PATH)\n",
    "            onnx.checker.check_model(onnx_model)\n",
    "            print(\"ONNX model check passed.\")\n",
    "\n",
    "            # Create ONNX Runtime session\n",
    "            # Specify providers based on your hardware (e.g., ['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if DEVICE == 'cuda' else ['CPUExecutionProvider']\n",
    "            available_providers = ort.get_available_providers()\n",
    "            valid_providers = [p for p in providers if p in available_providers]\n",
    "            if not valid_providers:\n",
    "                print(f\"Warning: None of the preferred providers {providers} are available. Using default.\")\n",
    "                valid_providers = None # Let ORT decide\n",
    "            \n",
    "            print(f\"Creating ONNX Runtime session with providers: {valid_providers}\")\n",
    "            ort_session = ort.InferenceSession(OUTPUT_ONNX_PATH, providers=valid_providers)\n",
    "            print(\"ONNX Runtime session created.\")\n",
    "\n",
    "            # Prepare input for ONNX Runtime (needs numpy arrays)\n",
    "            ort_inputs = {}\n",
    "            for key, tensor in example_normalized_input_device.items():\n",
    "                if isinstance(tensor, torch.Tensor):\n",
    "                     ort_inputs[key] = tensor.cpu().numpy()\n",
    "                else:\n",
    "                     # Handle non-tensor inputs if necessary (e.g. language might need specific format)\n",
    "                     ort_inputs[key] = tensor # Assuming it's already in a suitable format (like list of strings)\n",
    "                     # If model expects embeddings, they should be in example_normalized_input_device\n",
    "\n",
    "            # Run inference\n",
    "            print(\"Running inference with ONNX Runtime...\")\n",
    "            ort_outputs = ort_session.run(output_names, ort_inputs)\n",
    "            print(\"ONNX Runtime inference complete.\")\n",
    "\n",
    "            # Compare with PyTorch output (before denormalization)\n",
    "            with torch.no_grad():\n",
    "                 pytorch_normalized_action = policy._get_action_from_normalized_input(example_normalized_input_device)\n",
    "\n",
    "            # Assuming the first output is the action prediction\n",
    "            onnx_output_action = ort_outputs[0]\n",
    "            pytorch_output_action_np = pytorch_normalized_action.cpu().numpy()\n",
    "\n",
    "            # Check shapes\n",
    "            print(f\"PyTorch normalized output shape: {pytorch_output_action_np.shape}\")\n",
    "            print(f\"ONNX normalized output shape:    {onnx_output_action.shape}\")\n",
    "\n",
    "            # Check values (using tolerance due to potential floating point differences)\n",
    "            if np.allclose(pytorch_output_action_np, onnx_output_action, rtol=1e-3, atol=1e-4):\n",
    "                print(\"\\nVerification SUCCESS: ONNX Runtime output closely matches PyTorch output.\")\n",
    "            else:\n",
    "                diff = np.abs(pytorch_output_action_np - onnx_output_action)\n",
    "                print(\"\\nVerification WARNING: ONNX Runtime output differs from PyTorch output.\")\n",
    "                print(f\"  Max absolute difference: {np.max(diff)}\")\n",
    "                print(f\"  Mean absolute difference: {np.mean(diff)}\")\n",
    "                print(\"  This might be due to precision differences (e.g., BF16 vs FP32) or export issues.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during ONNX Runtime verification: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping ONNX verification because export did not succeed or ort is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps: Using the ONNX Model\n",
    "\n",
    "1.  **Integrate into your application:** Load the `gr00t_policy_model.onnx` file using your chosen ONNX runtime (ONNX Runtime, TensorRT, etc.).\n",
    "2.  **Implement Preprocessing:** Before feeding observations to the ONNX model, apply the exact same normalization steps as `policy.apply_transforms` does.\n",
    "3.  **Run Inference:** Pass the normalized input tensors to the ONNX model.\n",
    "4.  **Implement Postprocessing:** Take the normalized output tensors from the ONNX model and apply the inverse transformations (denormalization) equivalent to `policy.unapply_transforms` to get the final, usable actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/Isaac-GR00T/.venv/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.6 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Eagle2ChatConfig {\n",
      "  \"_commit_hash\": null,\n",
      "  \"_name_or_path\": \"/teamspace/studios/this_studio/Isaac-GR00T/gr00t/model/backbone/eagle2_hg_model\",\n",
      "  \"architectures\": [\n",
      "    \"Eagle2ChatModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_eagle_chat.Eagle2ChatConfig\",\n",
      "    \"AutoModel\": \"modeling_eagle_chat.Eagle2ChatModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_eagle_chat.Eagle2ChatModel\"\n",
      "  },\n",
      "  \"downsample_ratio\": 0.5,\n",
      "  \"dynamic_image_size\": true,\n",
      "  \"force_image_size\": 224,\n",
      "  \"keep_aspect_ratio\": false,\n",
      "  \"llm_config\": {\n",
      "    \"_name_or_path\": \"./pretrained/SmolLM2-1_7B-Instruct\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"auto_map\": {\n",
      "      \"AutoConfig\": \"configuration_llama.LlamaConfig\",\n",
      "      \"AutoModel\": \"modeling_llama.LlamaModel\",\n",
      "      \"AutoModelForCausalLM\": \"modeling_llama.LlamaForCausalLM\"\n",
      "    },\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 1,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 8192,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"min_length\": 0,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"my_rope_scaling\": null,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 2,\n",
      "    \"prefix\": null,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 130000,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"torchscript\": false,\n",
      "    \"transformers.js_config\": {\n",
      "      \"kv_cache_dtype\": {\n",
      "        \"fp16\": \"float16\",\n",
      "        \"q4f16\": \"float16\"\n",
      "      }\n",
      "    },\n",
      "    \"transformers_version\": \"4.45.2\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": false,\n",
      "    \"vocab_size\": 49164\n",
      "  },\n",
      "  \"loss_version\": \"efficient_v2_cp_head\",\n",
      "  \"max_dynamic_patch\": 12,\n",
      "  \"min_dynamic_patch\": 1,\n",
      "  \"mlp_checkpoint\": false,\n",
      "  \"model_type\": \"eagle_chat\",\n",
      "  \"pad2square\": false,\n",
      "  \"pre_feature_reduction\": false,\n",
      "  \"ps_version\": \"v2\",\n",
      "  \"select_layer\": -1,\n",
      "  \"template\": \"qwen2-chat\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": null,\n",
      "  \"use_backbone_lora\": 0,\n",
      "  \"use_llm_lora\": 0,\n",
      "  \"use_thumbnail\": true,\n",
      "  \"vision_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"drop_path_rate\": 0.1,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 1152,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"intermediate_size\": 4304,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-06,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"siglip_vision_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 27,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 14,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.45.2\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"vocab_size\": 49164\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid image: {'pil_image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x73CBEC7DF250>}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Prepare input using the processor\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# The processor expects a specific dictionary format\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     input_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     64\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# Add other params if needed, e.g., \"video_frame_num\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     }\n\u001b[0;32m---> 72\u001b[0m     vl_input_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Move tensors to the correct device\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     vl_input \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m vl_input_data\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Isaac-GR00T/gr00t/model/backbone/eagle2_hg_model/inference_eagle_repo.py:353\u001b[0m, in \u001b[0;36mEagleProcessor.prepare_input\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_data \u001b[38;5;129;01min\u001b[39;00m message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 353\u001b[0m         pil_images\u001b[38;5;241m.\u001b[39mappend(\u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    354\u001b[0m         prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_image_cnt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m><image>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         global_image_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Isaac-GR00T/gr00t/model/backbone/eagle2_hg_model/inference_eagle_repo.py:122\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mfromarray(image[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp_array\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid image: {'pil_image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x73CBEC7DF250>}"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Assuming eagle2_hg_model directory is accessible\n",
    "# Adjust model_name path if necessary\n",
    "from gr00t.model.backbone.eagle_backbone import EagleBackbone\n",
    "from gr00t.model.backbone.eagle2_hg_model.inference_eagle_repo import EagleProcessor, ModelSpecificValues\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust paths and settings as needed for your environment\n",
    "MODEL_PATH = \"/teamspace/studios/this_studio/Isaac-GR00T/gr00t/model/backbone/eagle2_hg_model\" # Or where you have the model files\n",
    "PROCESSOR_CFG = {\n",
    "    \"model_path\": MODEL_PATH,\n",
    "    \"max_input_tiles\": 1, # Example value, adjust based on config/needs\n",
    "    \"model_spec\": {\n",
    "        \"template\": \"qwen2-chat\", # From config.json\n",
    "        \"num_image_token\": 64    # From config.json (adjust if scale_image_resolution != 1)\n",
    "    }\n",
    "}\n",
    "IMAGE_URL = \"https://garden.spoonflower.com/c/14186636/i/m/DYzyp2VREexAT8IEVolsqQn52vqoyzlo2P8AI4IZqg0QOjVXtA/14186636-imgonline-com-ua-resize-zlsdjrcdmm1-by-ameetlad.jpg\" # Replace with an actual image URL\n",
    "QUESTION = \"Describe this image.\"\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\" # Or use the default from the processor\n",
    "\n",
    "# --- Initialization ---\n",
    "# Ensure you are running on CPU (PyTorch defaults if no GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate processor and backbone\n",
    "# Note: EagleBackbone constructor might need adjustments based on your exact needs\n",
    "# (e.g., select_layer, tune_llm, tune_visual, projector_dim etc.)\n",
    "processor = EagleProcessor(\n",
    "    model_path=PROCESSOR_CFG[\"model_path\"],\n",
    "    max_input_tiles=PROCESSOR_CFG[\"max_input_tiles\"],\n",
    "    model_spec=ModelSpecificValues(**PROCESSOR_CFG[\"model_spec\"]),\n",
    "    use_local_eagle_hg_model=False # Set to False if using a custom path\n",
    ")\n",
    "\n",
    "backbone = EagleBackbone(\n",
    "    model_name=MODEL_PATH,\n",
    "    processor_cfg=PROCESSOR_CFG,\n",
    "    use_local_eagle_hg_model=False # Set to False if using a custom path\n",
    ").to(device)\n",
    "backbone.eval() # Set to evaluation mode\n",
    "\n",
    "# --- Prepare Input ---\n",
    "# Load image (Example using requests)\n",
    "try:\n",
    "    response = requests.get(IMAGE_URL)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading image: {e}\")\n",
    "    # Handle error appropriately, maybe exit or use a placeholder\n",
    "    image = None # Or some default image\n",
    "\n",
    "if image:\n",
    "    # Prepare input using the processor\n",
    "    # The processor expects a specific dictionary format\n",
    "    input_params = {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": QUESTION, \"image\": [{\"pil_image\": image}]} # Pass PIL image directly if loaded\n",
    "            # Or use other formats supported by load_image in inference_eagle_repo.py\n",
    "            # {\"role\": \"user\", \"content\": QUESTION, \"image\": [{\"url\": IMAGE_URL}]}\n",
    "        ],\n",
    "        # Add other params if needed, e.g., \"video_frame_num\"\n",
    "    }\n",
    "\n",
    "    vl_input_data = processor.prepare_input(input_params)\n",
    "\n",
    "    # Move tensors to the correct device\n",
    "    vl_input = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in vl_input_data.items()}\n",
    "    vl_input = backbone.prepare_input(vl_input) # Wrap in BatchFeature\n",
    "\n",
    "    # --- Run Forward Pass ---\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = backbone(vl_input)\n",
    "            print(\"Backbone output features shape:\", output[\"backbone_features\"].shape)\n",
    "            # You can now use output[\"backbone_features\"] and output[\"backbone_attention_mask\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            # Add more detailed error handling/debugging if needed\n",
    "else:\n",
    "    print(\"Cannot proceed without a valid image.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gr00t\n",
    "\n",
    "DEFAULT_EAGLE_MODEL_NAME = os.path.join(\n",
    "    os.path.dirname(gr00t.__file__), \"model\", \"backbone\", \"eagle2_hg_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/Isaac-GR00T/gr00t/model/backbone/eagle2_hg_model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_EAGLE_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "config = AutoConfig.from_pretrained(DEFAULT_EAGLE_MODEL_NAME, trust_remote_code=True)\n",
    "config._attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mneftune_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Isaac-GR00T/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:437\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    436\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    439\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/Isaac-GR00T/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1519\u001b[0m, in \u001b[0;36mPreTrainedModel._from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;66;03m# restore default dtype if it was modified\u001b[39;00m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/.cache/huggingface/modules/transformers_modules/eagle2_hg_model/modeling_eagle_chat.py:72\u001b[0m, in \u001b[0;36mEagle2ChatModel.__init__\u001b[0;34m(self, config, vision_model, language_model)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mvision_config\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiglip_vision_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model \u001b[38;5;241m=\u001b[39m \u001b[43mSiglipVisionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model \u001b[38;5;241m=\u001b[39m language_model\n",
      "File \u001b[0;32m~/Isaac-GR00T/.venv/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:1145\u001b[0m, in \u001b[0;36mSiglipVisionModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: SiglipVisionConfig):\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model \u001b[38;5;241m=\u001b[39m SiglipVisionTransformer(config)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/Isaac-GR00T/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1404\u001b[0m, in \u001b[0;36mPreTrainedModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1399\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter config in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(config)` should be an instance of class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PretrainedConfig`. To create a model from a pretrained model use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1402\u001b[0m     )\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;66;03m# Save config and origin of the pretrained weights if given in model\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mname_or_path\n",
      "File \u001b[0;32m~/Isaac-GR00T/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1572\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1572\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;66;03m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_sdpa(\n\u001b[1;32m   1582\u001b[0m         config,\n\u001b[1;32m   1583\u001b[0m         hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1584\u001b[0m     )\n",
      "File \u001b[0;32m~/Isaac-GR00T/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1699\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[1;32m   1696\u001b[0m install_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1699\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1701\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_config(\n",
    "    config,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    ")\n",
    "print(model.config)\n",
    "model.neftune_alpha = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
